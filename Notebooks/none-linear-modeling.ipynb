{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear regression \n",
    "Nonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. Simple linear regression relates two variables (X and Y) with a straight line (y = mx + b), while nonlinear regression relates the two variables in a nonlinear (curved) relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to do everything that we did before so refere to Linear-Regression notebook for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'style', 'Exterior color', 'interior color', 'Engine',\n",
       "       'drive type', 'Fuel Type', 'Transmission', 'Mileage', 'mpg city',\n",
       "       'mpg highway', 'price', 'Year', 'Engine V', 'Brand'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars =  pd.read_csv(\"cleaned_data.csv\")\n",
    "cars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  cars[['Name', 'style', 'Exterior color', 'interior color', 'Engine',\n",
    "       'drive type', 'Fuel Type', 'Transmission', 'Mileage', 'mpg city',\n",
    "       'mpg highway', 'Year', 'Engine V', 'Brand']]\n",
    "\n",
    "\n",
    "Y = cars[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "\n",
    "categorical_features = onehot.fit_transform(X.iloc[:, [1,4,5,6,7,13]]).toarray()\n",
    "X = np.delete(X.values, [0,1,2,3,4,5,6,7,13], 1)\n",
    "X = np.concatenate((X,categorical_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y,\n",
    "    test_size=0.1,\n",
    "    random_state=82\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(x_train)\n",
    "\n",
    "x_train_std = std_scaler.transform(x_train)\n",
    "x_test_std  = std_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate polynomial and interaction features :\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.787403266469926e+21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "x_train_quad = quadratic.fit_transform(x_train_std)\n",
    "x_test_quad = quadratic.transform(x_test_std)\n",
    "\n",
    "lr.fit(x_train_quad, y_train)\n",
    "\n",
    "y_pred_poly = lr.predict(x_test_quad)\n",
    "print(r2_score(y_test, y_pred_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">read documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8782040921032037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(criterion=\"squared_error\", splitter=\"best\")\n",
    "dtr.fit(x_train_std, y_train)\n",
    "\n",
    "y_pred_tree = dtr.predict(x_test_std)\n",
    "print(r2_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see w've got a little bit of more accuracy with DecisionTreeRegressor that is beacause of none linear fit of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sina Kazemi<br>\n",
    "Github : <a href=\"https://github.com/sina96n/\">sina96n</a>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
