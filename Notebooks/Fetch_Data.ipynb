{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data\n",
    "In this notebook we extract all the liks 0f first 250 page of <a href=\"https://www.truecar.com\" >Truecar</a> website. the number of pages can be changed with MAX_PAGE variable.\n",
    "after the page's extraction, we have to extract all the urls of the car ads so we can get the information of each car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PAGE  = 250\n",
    "# defining base_url to create a list of all page urls\n",
    "BASE_URL  = \"https://www.truecar.com/used-cars-for-sale/listings/\"\n",
    "# defining host_name for creating urls for each car ad\n",
    "HOST_NAME = \"https://www.truecar.com\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this part we are going to create a list of all page urls and loop throuhg all pages to extract all urls of the cars.<br>\n",
    "\n",
    "output files :<br>\n",
    "\n",
    "* **pages.txt** : containing all the page urls<br>\n",
    "* **urls.txt** : containing all the urls<br>\n",
    "> Pages algorithm: https://www.truecar.com/used-cars-for-sale/listings/?page=num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list()\n",
    "urls = list()\n",
    "failed_pages = list()\n",
    "\n",
    "def url_scraper():\n",
    "    with open(\"pages.txt\", \"w\") as f:\n",
    "        for i in range(1,MAX_PAGE+1):\n",
    "            page = BASE_URL + \"?page=\" + str(i)\n",
    "            f.write(page+\"\\n\")\n",
    "            pages.append(page)\n",
    "\n",
    "    for page in pages:\n",
    "        try:\n",
    "            response = requests.get(page)\n",
    "            response.raise_for_status()\n",
    "        except:\n",
    "            failed_pages.append(page)\n",
    "            continue\n",
    "\n",
    "        src = response.text\n",
    "        soup = BeautifulSoup(src, \"html.parser\")\n",
    "\n",
    "        ads = soup.find_all(\"a\", attrs={\n",
    "            \"data-test\" : \"vehicleCardLink\"\n",
    "        })\n",
    "         \n",
    "        url_list = [HOST_NAME+link[\"href\"] for link in ads]\n",
    "        with open(\"urls.txt\", \"a+\") as f:\n",
    "            for url in url_list:\n",
    "                urls.append(url)\n",
    "                f.write(url+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main scraping part happens in this part. we loop through all car urls and try to extract its information.\n",
    "<br>\n",
    "> **Notice** : this is a long process. make sure you have a stable internet connection and dont forget to run the next block of code before closing notebook to save extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_list = ()\n",
    "failed_urls = list()\n",
    "\n",
    "def scraper(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except:\n",
    "        failed_urls.append(url)\n",
    "        pass\n",
    "\n",
    "        try:\n",
    "            src = response.text\n",
    "            soup = BeautifulSoup(src, \"html.parser\")\n",
    "\n",
    "            detail = soup.find(\"div\", attrs={\"data-test\" : \"vdpPreProspectTopDetails\"})\n",
    "\n",
    "            price = detail.find(\"div\", attrs={\"data-test\" : \"vdpPreProspectPrice\"})\n",
    "            price = int(re.sub(\"[^\\d]\", \"\", price.text))\n",
    "\n",
    "            mileage = detail.find(\"p\", attrs={\"class\" : \"margin-top-1\"})\n",
    "            mileage = int(re.sub(\"[^\\d]\", \"\", mileage.text))\n",
    "\n",
    "            titles = soup.find(\"h1\", attrs={\n",
    "                \"class\" : \"heading-base d-flex flex-column margin-right-2\",\n",
    "                \"data-qa\" : \"Heading\"\n",
    "            })\n",
    "            name = list(titles.children)[1].text\n",
    "            name = name.replace(\"\\xa0\",\" \")\n",
    "\n",
    "            features = list(soup.find(\"div\", attrs={\"data-test\" : \"vdpOverviewSection\"}).div)\n",
    "            style = features[0].div.div.p.text\n",
    "            exterior_color = features[1].div.div.p.text\n",
    "            interior_color = features[2].div.div.p.text\n",
    "            mpg = features[3].div.div.p.text\n",
    "            mpg_city, mpg_highway = re.match(\"(\\d{1,2}) cty / (\\d{1,2}) hwy\", mpg).groups()\n",
    "            engine = features[4].div.div.p.text\n",
    "            drive_type = features[5].div.div.p.text\n",
    "            fuel_type = features[6].div.div.p.text\n",
    "            transmission = features[7].div.div.p.text\n",
    "\n",
    "        except:\n",
    "            failed_urls.append(url)\n",
    "            pass\n",
    "            \n",
    " \n",
    "        car = [name,style,exterior_color,interior_color,engine,\n",
    "                    drive_type,fuel_type,transmission,mileage,mpg_city,\n",
    "                    mpg_highway,price]\n",
    "\n",
    "        car_list.append(car)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exporting a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cars.csv\", \"w\") as cars:\n",
    "    csvwriter = csv.writer(cars)\n",
    "    csvwriter.writerows(car_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sina Kazemi<br>\n",
    "Github : <a href=\"https://github.com/sina96n/\">sina96n</a>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cea63f0ceb6f9dc02c27b329dbcce9c31ce16d7e30233a74372b554b8e264989"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
